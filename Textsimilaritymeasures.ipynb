{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7K_q2AvpwYB2",
        "outputId": "d02ca08c-08fb-413b-dfa9-6b0f62bf8217"
      },
      "source": [
        "import nltk\n",
        "import nltk.translate.bleu_score as bleu\n",
        "import nltk.translate.gleu_score as gleu\n",
        "import math\n",
        "import numpy as np\n",
        "import os\n",
        "try:\n",
        "  nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "  nltk.download('punkt')\n",
        "\n",
        "!pip install bert-embedding\n",
        "from bert_embedding import BertEmbedding\n",
        "bert_E = BertEmbedding()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Collecting bert-embedding\n",
            "  Downloading https://files.pythonhosted.org/packages/62/85/e0d56e29a055d8b3ba6da6e52afe404f209453057de95b90c01475c3ff75/bert_embedding-1.0.1-py3-none-any.whl\n",
            "Collecting numpy==1.14.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/c4/395ebb218053ba44d64935b3729bc88241ec279915e72100c5979db10945/numpy-1.14.6-cp36-cp36m-manylinux1_x86_64.whl (13.8MB)\n",
            "\u001b[K     |████████████████████████████████| 13.8MB 266kB/s \n",
            "\u001b[?25hCollecting mxnet==1.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/e9/241aadccc4522f99adee5b6043f730d58adb7c001e0a68865a3728c3b4ae/mxnet-1.4.0-py2.py3-none-manylinux1_x86_64.whl (29.6MB)\n",
            "\u001b[K     |████████████████████████████████| 29.6MB 118kB/s \n",
            "\u001b[?25hCollecting typing==3.6.6\n",
            "  Downloading https://files.pythonhosted.org/packages/4a/bd/eee1157fc2d8514970b345d69cb9975dcd1e42cd7e61146ed841f6e68309/typing-3.6.6-py3-none-any.whl\n",
            "Collecting gluonnlp==0.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e2/07/037585c23bccec19ce333b402997d98b09e43cc8d2d86dc810d57249c5ff/gluonnlp-0.6.0.tar.gz (209kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 54.9MB/s \n",
            "\u001b[?25hCollecting graphviz<0.9.0,>=0.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/53/39/4ab213673844e0c004bed8a0781a0721a3f6bb23eb8854ee75c236428892/graphviz-0.8.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet==1.4.0->bert-embedding) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet==1.4.0->bert-embedding) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet==1.4.0->bert-embedding) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet==1.4.0->bert-embedding) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet==1.4.0->bert-embedding) (2020.11.8)\n",
            "Building wheels for collected packages: gluonnlp\n",
            "  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gluonnlp: filename=gluonnlp-0.6.0-cp36-none-any.whl size=259917 sha256=0c7954f68a1803b9da745a0f6da9810027cf9552ea9956e56c0d10915c8107cf\n",
            "  Stored in directory: /root/.cache/pip/wheels/ff/48/ac/a77c79aa416ba6dd7bf487f2280b0471034f66141617965914\n",
            "Successfully built gluonnlp\n",
            "\u001b[31mERROR: xarray 0.15.1 has requirement numpy>=1.15, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: umap-learn 0.4.6 has requirement numpy>=1.17, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tifffile 2020.9.3 has requirement numpy>=1.15.1, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement numpy<1.19.0,>=1.16.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: spacy 2.2.4 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: seaborn 0.11.0 has requirement numpy>=1.15, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: plotnine 0.6.0 has requirement numpy>=1.16.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pandas 1.1.4 has requirement numpy>=1.15.4, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: numba 0.48.0 has requirement numpy>=1.15, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: imgaug 0.2.9 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fbprophet 0.7.1 has requirement numpy>=1.15.4, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fastai 1.0.61 has requirement numpy>=1.15, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: cvxpy 1.0.31 has requirement numpy>=1.15, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: blis 0.4.1 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: astropy 4.1 has requirement numpy>=1.16, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy, graphviz, mxnet, typing, gluonnlp, bert-embedding\n",
            "  Found existing installation: numpy 1.18.5\n",
            "    Uninstalling numpy-1.18.5:\n",
            "      Successfully uninstalled numpy-1.18.5\n",
            "  Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "Successfully installed bert-embedding-1.0.1 gluonnlp-0.6.0 graphviz-0.8.4 mxnet-1.4.0 numpy-1.14.6 typing-3.6.6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "typing"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Vocab file is not found. Downloading.\n",
            "Downloading /root/.mxnet/models/book_corpus_wiki_en_uncased-a6607397.zip from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/vocab/book_corpus_wiki_en_uncased-a6607397.zip...\n",
            "Downloading /root/.mxnet/models/bert_12_768_12_book_corpus_wiki_en_uncased-75cc780f.zip from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/models/bert_12_768_12_book_corpus_wiki_en_uncased-75cc780f.zip...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRHR1HpDwebc",
        "outputId": "11e5d3e5-93ea-4dd7-ab5f-e087b0ae7ac0"
      },
      "source": [
        "!python -m spacy download pt_core_news_sm\n",
        "\n",
        "import pt_core_news_sm\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nlp = pt_core_news_sm.load()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[38;5;3m⚠ Skipping model package dependencies and setting `--no-deps`. You\n",
            "don't seem to have the spaCy package itself installed (maybe because you've\n",
            "built from source?), so installing the model dependencies would cause spaCy to\n",
            "be downloaded, which probably isn't what you want. If the model package has\n",
            "other dependencies, you'll have to install them manually.\u001b[0m\n",
            "Collecting pt_core_news_sm==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-2.2.5/pt_core_news_sm-2.2.5.tar.gz (21.2MB)\n",
            "\u001b[K     |████████████████████████████████| 21.2MB 1.2MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pt-core-news-sm\n",
            "  Building wheel for pt-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pt-core-news-sm: filename=pt_core_news_sm-2.2.5-cp36-none-any.whl size=21186282 sha256=f0862570dc6b6d4175ced2854f60c5dfb8a02c3123c3aa737f875720e5b6cb74\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-eo5a2j86/wheels/ea/94/74/ec9be8418e9231b471be5dc7e1b45dd670019a376a6b5bc1c0\n",
            "Successfully built pt-core-news-sm\n",
            "Installing collected packages: pt-core-news-sm\n",
            "Successfully installed pt-core-news-sm-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('pt_core_news_sm')\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IbTgCulwia9"
      },
      "source": [
        "def token_list(embeddings, no_sep=False):\n",
        "    \"\"\"\n",
        "    Returns with the tokens of the embedding data from the BertEmbedding.\n",
        "\n",
        "    Params:\n",
        "        embeddings: The embedding data from BertEmbedding\n",
        "        no_sep: If True, the separators are trimmed.\n",
        "    Return:\n",
        "        tokens: list of tokens\n",
        "    \"\"\"\n",
        "    if no_sep:\n",
        "        return embeddings[0][0][1:-1]\n",
        "    return embeddings[0][0]\n",
        "\n",
        "\n",
        "def sentence_embs(embeddings):\n",
        "    \"\"\"Return with the sentence level embeddings\"\"\"\n",
        "    return embeddings[0][1][0]\n",
        "\n",
        "def prep(sentence):\n",
        "    \"\"\"Return with tokens and sentence level embeddings\"\"\"\n",
        "    embs = bert_E([sentence])\n",
        "    tokens = token_list(embs, no_sep=True)\n",
        "    se = sentence_embs(embs)\n",
        "    return tokens, se\n",
        "\n",
        "\n",
        "def mapfunct(x, type='exp', n=0.2):\n",
        "    \"\"\"\n",
        "    Map 0-inf to 1-0 with some function\n",
        "    \n",
        "    Type:\n",
        "        inverse: 1/(1+n*x)\n",
        "        arctan: 1-2/pi*arctan(x)\n",
        "        exp: (1/(1+n))^x\n",
        "    \"\"\"\n",
        "    if type=='inverse':\n",
        "        return 1/(1+n*x)\n",
        "    if type=='arctan':\n",
        "        return 1-2/math.pi*math.atan(n*x)\n",
        "    if type=='exp':\n",
        "        return (1/(1+n))**x\n",
        "    else:\n",
        "        raise(NotImplementedError(\"Function not implemented\"))\n",
        "\n",
        "\n",
        "def square_rooted(x):\n",
        "    return math.sqrt(sum([a*a for a in x]))\n",
        "\n",
        "\n",
        "def cosine_similarity(x,y):\n",
        "    numerator = sum(a*b for a,b in zip(x,y))\n",
        "    denominator = square_rooted(x)*square_rooted(y)\n",
        "    return numerator/float(denominator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohg_e8n9wmkd"
      },
      "source": [
        "s0 = \"James Cook was a very good man and a loving husband.\"\n",
        "s1 = \"James Cook was a very nice man and a loving husband.\"\n",
        "s2 = \"James Cook was a bad man and a terrible husband.\"\n",
        "s3 = \"James Cook was a nice person and a good husband.\"\n",
        "s4 = \"The sky is blue today and learning history is important.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNhnF6bqwtLx"
      },
      "source": [
        "r0, e0 = prep(s0)\n",
        "r1, e1 = prep(s1)\n",
        "r2, e2 = prep(s2)\n",
        "r3, e3 = prep(s3)\n",
        "r4, e4 = prep(s4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AydF9AoRwtw0"
      },
      "source": [
        "t0 = nlp(s0)\n",
        "t1 = nlp(s1)\n",
        "t2 = nlp(s2)\n",
        "t3 = nlp(s3)\n",
        "t4 = nlp(s4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qS2yIgswwjB"
      },
      "source": [
        "SmoothingFunction = nltk.translate.bleu_score.SmoothingFunction()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LXQWDTHwzfO",
        "outputId": "666c72d8-8878-4bee-e82e-ded9cfac71a4"
      },
      "source": [
        "print(\"r0-r0 bleu score: \", bleu.sentence_bleu([r0], r0, smoothing_function=SmoothingFunction.method0))\n",
        "print(\"r0-r1 bleu score: \", bleu.sentence_bleu([r1], r0, smoothing_function=SmoothingFunction.method0))\n",
        "print(\"r0-r2 bleu score: \", bleu.sentence_bleu([r2], r0, smoothing_function=SmoothingFunction.method0))\n",
        "print(\"r0-r3 bleu score: \", bleu.sentence_bleu([r3], r0, smoothing_function=SmoothingFunction.method0))\n",
        "print(\"r0-r4 bleu score: \", bleu.sentence_bleu([r4], r0, smoothing_function=SmoothingFunction.method0))\n",
        "print(\"\")\n",
        "print(\"r0-r0 bleu score: \", bleu.sentence_bleu([r0], r0, smoothing_function=SmoothingFunction.method2))\n",
        "print(\"r0-r1 bleu score: \", bleu.sentence_bleu([r1], r0, smoothing_function=SmoothingFunction.method2))\n",
        "print(\"r0-r2 bleu score: \", bleu.sentence_bleu([r2], r0, smoothing_function=SmoothingFunction.method2))\n",
        "print(\"r0-r3 bleu score: \", bleu.sentence_bleu([r3], r0, smoothing_function=SmoothingFunction.method2))\n",
        "print(\"r0-r4 bleu score: \", bleu.sentence_bleu([r4], r0, smoothing_function=SmoothingFunction.method2))\n",
        "print(\"\")\n",
        "print(\"r0-r0 gleu score: \", gleu.sentence_gleu([r0], r0))\n",
        "print(\"r0-r1 gleu score: \", gleu.sentence_gleu([r1], r0))\n",
        "print(\"r0-r2 gleu score: \", gleu.sentence_gleu([r2], r0))\n",
        "print(\"r0-r3 gleu score: \", gleu.sentence_gleu([r3], r0))\n",
        "print(\"r0-r4 gleu score: \", gleu.sentence_gleu([r4], r0))\n",
        "print(\"\")\n",
        "print(\"e0-e0 Euclid distance:\", np.linalg.norm(e0-e0))\n",
        "print(\"e0-e1 Euclid distance:\", np.linalg.norm(e1-e0))\n",
        "print(\"e0-e2 Euclid distance:\", np.linalg.norm(e2-e0))\n",
        "print(\"e0-e3 Euclid distance:\", np.linalg.norm(e3-e0))\n",
        "print(\"e0-e4 Euclid distance:\", np.linalg.norm(e4-e0))\n",
        "print(\"\")\n",
        "print(\"e0-e0 Euclid distance:\", str(mapfunct(np.linalg.norm(e0-e0))))\n",
        "print(\"e0-e1 Euclid distance:\", str(mapfunct(np.linalg.norm(e1-e0))))\n",
        "print(\"e0-e2 Euclid distance:\", str(mapfunct(np.linalg.norm(e2-e0))))\n",
        "print(\"e0-e3 Euclid distance:\", str(mapfunct(np.linalg.norm(e3-e0))))\n",
        "print(\"e0-e4 Euclid distance:\", str(mapfunct(np.linalg.norm(e4-e0))))\n",
        "print(\"\")\n",
        "print(\"e0-e0 cosine-similarity:\", cosine_similarity(e0,e0))\n",
        "print(\"e0-e1 cosine-similarity:\", cosine_similarity(e1,e0))\n",
        "print(\"e0-e2 cosine-similarity:\", cosine_similarity(e2,e0))\n",
        "print(\"e0-e3 cosine-similarity:\", cosine_similarity(e3,e0))\n",
        "print(\"e0-e4 cosine-similarity:\", cosine_similarity(e4,e0))\n",
        "print(\"\")\n",
        "print(\"t0-t0 spacy similarity\", t0.similarity(t0))\n",
        "print(\"t0-t1 spacy similarity\", t1.similarity(t0))\n",
        "print(\"t0-t2 spacy similarity\", t2.similarity(t0))\n",
        "print(\"t0-t3 spacy similarity\", t3.similarity(t0))\n",
        "print(\"t0-t4 spacy similarity\", t4.similarity(t0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "r0-r0 bleu score:  1.0\n",
            "r0-r1 bleu score:  0.6580370064762462\n",
            "r0-r2 bleu score:  0.5280972216470737\n",
            "r0-r3 bleu score:  0.4132584091896901\n",
            "r0-r4 bleu score:  0.5623413251903491\n",
            "\n",
            "r0-r0 bleu score:  1.0\n",
            "r0-r1 bleu score:  0.7016879391277372\n",
            "r0-r2 bleu score:  0.3508439695638686\n",
            "r0-r3 bleu score:  0.2998221389342337\n",
            "r0-r4 bleu score:  0.12605968092174913\n",
            "\n",
            "r0-r0 gleu score:  1.0\n",
            "r0-r1 gleu score:  0.7058823529411765\n",
            "r0-r2 gleu score:  0.38235294117647056\n",
            "r0-r3 gleu score:  0.3235294117647059\n",
            "r0-r4 gleu score:  0.029411764705882353\n",
            "\n",
            "e0-e0 Euclid distance: 0.0\n",
            "e0-e1 Euclid distance: 1.9738714\n",
            "e0-e2 Euclid distance: 3.6317627\n",
            "e0-e3 Euclid distance: 3.0969253\n",
            "e0-e4 Euclid distance: 17.017267\n",
            "\n",
            "e0-e0 Euclid distance: 1.0\n",
            "e0-e1 Euclid distance: 0.6977605424333337\n",
            "e0-e2 Euclid distance: 0.5157420006874345\n",
            "e0-e3 Euclid distance: 0.568566934219589\n",
            "e0-e4 Euclid distance: 0.04493156820402338\n",
            "\n",
            "e0-e0 cosine-similarity: 1.0\n",
            "e0-e1 cosine-similarity: 0.9900622593588156\n",
            "e0-e2 cosine-similarity: 0.965961241983015\n",
            "e0-e3 cosine-similarity: 0.9760124353647561\n",
            "e0-e4 cosine-similarity: 0.30749654649663904\n",
            "\n",
            "t0-t0 spacy similarity 1.0\n",
            "t0-t1 spacy similarity 0.9886800132120467\n",
            "t0-t2 spacy similarity 0.9586492042474178\n",
            "t0-t3 spacy similarity 0.9613467509851176\n",
            "t0-t4 spacy similarity 0.8462854821492628\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABeEv55hw17L"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}